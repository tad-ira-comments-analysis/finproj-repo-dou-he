{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regulations.gov: Comment Embeddings >>> Clustering >>> Post-processing\n",
        "\n",
        "This notebook takes a cleaned Regulations.gov comment dataset and produces cluster labels plus cluster-level summaries for downstream text analysis (e.g., LDA / Wordfish).\n",
        "\n",
        "**Pipeline:**\n",
        "1. Loads a CSV of cleaned comments (must include a text column).\n",
        "2. Computes SBERT embeddings for each comment.\n",
        "3. Clusters comments (DBSCAN or graph connected-components at a similarity threshold).\n",
        "4. Keeps **all original columns** and adds a cluster label column.\n",
        "5. Generates:\n",
        "   - a cluster size table\n",
        "   - a de-duplicated “one row per cluster” dataset\n",
        "   - optional exports for later modeling\n",
        "\n",
        "> **Note:** You probably need to change `DATA_PATH` below to match your own file location."
      ],
      "metadata": {
        "id": "gGVXCGN6jjXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 0. Setup\n",
        "\n",
        "This block prepares the Colab environment.\n",
        "\n",
        "**What this block does**\n",
        "1. Checks GPU availability (`nvidia-smi`).\n",
        "2. Installs required packages.\n",
        "3. Imports libraries used later.\n",
        "4. Downloads NLTK tokenizers (only needed if you use sentence splitting later).\n",
        "5. Mounts Google Drive.\n",
        "6. Defines all input/output paths.\n",
        "\n",
        "> **Note:** Edit `BASE_DIR` and `CSV_FILENAME` to match your Drive folder and file name."
      ],
      "metadata": {
        "id": "BZRekZHGndCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 0.1 GPU check ----\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "xdRuwZT0nhtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 0.2 Install dependencies ----\n",
        "!pip install -q sentence-transformers nltk pandas scikit-learn tqdm\n",
        "\n",
        "# ---- 0.3 Imports ----\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "Ympi-REtoW5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 0.4 NLTK (for embedding) ----\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "id": "2MjDwVSjoacp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 0.5 Mount Google Drive ----\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "id": "UUbW4CBsoxe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 0.6 Project paths ----\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/<your_project>\"  # <-- EDIT to your drive\n",
        "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
        "\n",
        "PREFIX = \"irs_multi\"  # <-- EDIT\n",
        "\n",
        "# Input\n",
        "CSV_FILENAME = f\"{PREFIX}_comments_all_clean_irs_multi.csv\"  # <-- EDIT\n",
        "CSV_PATH = os.path.join(DATA_DIR, CSV_FILENAME)\n",
        "\n",
        "# Outputs\n",
        "EMBED_JSON_PATH = os.path.join(DATA_DIR, f\"{PREFIX}_comments_all_clean_irs_multi_embeddings_allmpnet_SL.json\")\n",
        "DBSCAN_CSV_PATH = os.path.join(DATA_DIR, f\"{PREFIX}_comments_all_clean_irs_multi_dbscan.csv\")\n",
        "DBSCAN_FULL_CSV_PATH = os.path.join(DATA_DIR, f\"{PREFIX}_comments_all_clean_irs_multi_dbscan_full.csv\")\n",
        "\n",
        "print(\"CSV_PATH:\", CSV_PATH)\n",
        "print(\"EMBED_JSON_PATH:\", EMBED_JSON_PATH)\n",
        "print(\"DBSCAN_CSV_PATH:\", DBSCAN_CSV_PATH)\n",
        "print(\"DBSCAN_FULL_CSV_PATH:\", DBSCAN_FULL_CSV_PATH)"
      ],
      "metadata": {
        "id": "MyUl1-H5oz2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load the CSV and standardize required columns\n",
        "\n",
        "We load the cleaned comments CSV and keep only the columns needed for the embedding pipeline.\n",
        "\n",
        "**Expected columns**\n",
        "- `commentId`: unique identifier for each comment\n",
        "- `text_clean`: cleaned text used for sentence splitting and embedding\n",
        "\n",
        "**What this step produces**\n",
        "- A DataFrame `df` containing only:\n",
        "  - `commentId` (as string)\n",
        "  - `text_clean` (as string, with missing values filled as empty strings)"
      ],
      "metadata": {
        "id": "-zQo3otfqt24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 1.1 Read CSV ----\n",
        "df_raw = pd.read_csv(CSV_PATH)\n",
        "\n",
        "print(f\"Loaded CSV with {df_raw.shape[0]:,} rows and {df_raw.shape[1]:,} columns.\")\n",
        "print(\"First 10 column names:\", list(df_raw.columns)[:10])\n",
        "\n",
        "# ---- 1.2 Check required columns ----\n",
        "required_cols = [\"commentId\", \"text_clean\"]\n",
        "missing = [c for c in required_cols if c not in df_raw.columns]\n",
        "if missing:\n",
        "    raise ValueError(\n",
        "        f\"Missing required column(s): {missing}\\n\"\n",
        "        f\"Available columns include: {list(df_raw.columns)[:30]}\"\n",
        "    )\n",
        "\n",
        "# ---- 1.3 Keep only columns we need ----\n",
        "df = df_raw[required_cols].copy()\n",
        "\n",
        "# ---- 1.4 Standardize types ----\n",
        "df[\"commentId\"] = df[\"commentId\"].astype(str)\n",
        "df[\"text_clean\"] = df[\"text_clean\"].fillna(\"\").astype(str)\n",
        "\n",
        "print(\"After selecting/cleaning columns:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "qN3n1An4qwhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Sentence-level SBERT embeddings\n",
        "\n",
        "Instead of embedding the full document directly, we:\n",
        "1. Split each comment into sentences (`nltk.sent_tokenize`).\n",
        "2. Embed each sentence using SBERT.\n",
        "3. Average sentence embeddings to obtain a single **document embedding** per comment.\n",
        "\n",
        "**Key settings**\n",
        "- Model: `sentence-transformers/all-mpnet-base-v2`\n",
        "- `MAX_SEQ_LENGTH`: truncation length *per sentence* (not per document)\n",
        "- `min_sent_len`: drop extremely short sentences to reduce noise\n",
        "\n",
        "**Output**\n",
        "- A function `embed_document_sentence_level(text)` that returns a 1D NumPy array\n",
        "  with shape `(embedding_dim,)`."
      ],
      "metadata": {
        "id": "wOsh25EXrDPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 2.1 Model setup ----\n",
        "from typing import List\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "MAX_SEQ_LENGTH = 128  # truncation applies to each sentence\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "model = SentenceTransformer(MODEL_NAME, device=device)\n",
        "model.max_seq_length = MAX_SEQ_LENGTH  # sentence-level truncation\n",
        "\n",
        "\n",
        "# ---- 2.2 Sentence-level embedding function ----\n",
        "def embed_document_sentence_level(text: str, min_sent_len: int = 5) -> np.ndarray:\n",
        "\n",
        "    # Normalize whitespace\n",
        "    text = \" \".join(str(text).split())\n",
        "    if not text:\n",
        "        return np.zeros(model.get_sentence_embedding_dimension(), dtype=np.float32)\n",
        "\n",
        "    # 1) Sentence splitting\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # 2) Filter very short sentences (noise reduction)\n",
        "    sentences = [s.strip() for s in sentences if len(s.strip()) >= min_sent_len]\n",
        "    if not sentences:\n",
        "        return np.zeros(model.get_sentence_embedding_dimension(), dtype=np.float32)\n",
        "\n",
        "    # 3) Encode all sentences (batching handled internally by SentenceTransformer)\n",
        "    sent_embeddings = model.encode(\n",
        "        sentences,\n",
        "        show_progress_bar=False,\n",
        "        convert_to_numpy=True\n",
        "    )\n",
        "\n",
        "    # 4) Mean pooling -> document embedding\n",
        "    doc_embedding = sent_embeddings.mean(axis=0).astype(np.float32)\n",
        "\n",
        "    return doc_embedding"
      ],
      "metadata": {
        "id": "4lrooAfTrE7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Compute sentence-level embeddings and export to JSON\n",
        "\n",
        "We compute one embedding per comment using `embed_document_sentence_level()` and save results to a JSON file.\n",
        "\n",
        "**What this step does**\n",
        "- Iterates over `text_clean`\n",
        "- Produces an embedding matrix with shape `(N, dim)`\n",
        "- Saves a record-per-row JSON (portable and easy to reload)\n",
        "\n",
        "**Output file**\n",
        "- `EMBED_JSON_PATH`: JSON list of records with:\n",
        "  - `commentId`\n",
        "  - `text_clean`\n",
        "  - `embedding`"
      ],
      "metadata": {
        "id": "rysmm9BjrkcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 3.1 Compute embeddings ----\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "comment_ids = df[\"commentId\"].tolist()\n",
        "texts = df[\"text_clean\"].tolist()\n",
        "\n",
        "use_cache = True # Prep for possibel runtime disconnect\n",
        "\n",
        "if use_cache and os.path.exists(EMBED_JSON_PATH):\n",
        "    print(\"Found cached embeddings JSON. Loading:\", EMBED_JSON_PATH)\n",
        "    embed_df = pd.read_json(EMBED_JSON_PATH, orient=\"records\")\n",
        "    embeddings = np.vstack(embed_df[\"embedding\"].apply(np.array).values).astype(np.float32)\n",
        "    print(\"Loaded embedding matrix with shape:\", embeddings.shape)\n",
        "\n",
        "else:\n",
        "    print(f\"Computing sentence-level embeddings for {len(texts):,} comments...\")\n",
        "\n",
        "    embeddings_list = []\n",
        "    for txt in tqdm(texts, total=len(texts)):\n",
        "        emb = embed_document_sentence_level(txt)\n",
        "        embeddings_list.append(emb)\n",
        "\n",
        "    embeddings = np.vstack(embeddings_list).astype(np.float32)  # (N, dim)\n",
        "    print(\"Embedding matrix shape:\", embeddings.shape)\n",
        "\n",
        "    # Build export table\n",
        "    embed_df = pd.DataFrame({\n",
        "        \"commentId\": comment_ids,\n",
        "        \"text_clean\": texts,\n",
        "        \"embedding\": embeddings.tolist()\n",
        "    })\n",
        "\n",
        "    # ---- 3.2 Save JSON ----\n",
        "    os.makedirs(os.path.dirname(EMBED_JSON_PATH), exist_ok=True)\n",
        "    tmp_path = EMBED_JSON_PATH + \".tmp\"\n",
        "\n",
        "    embed_df.to_json(tmp_path, orient=\"records\", force_ascii=False)\n",
        "    os.replace(tmp_path, EMBED_JSON_PATH)\n",
        "\n",
        "    print(\"Saved embeddings to:\", EMBED_JSON_PATH)\n",
        "\n",
        "embed_df.head(3)"
      ],
      "metadata": {
        "id": "w6kM7Eg5rrxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. DBSCAN clustering on comment embeddings\n",
        "\n",
        "This section runs DBSCAN on comment embeddings and exports both:\n",
        "- a lightweight cluster table, and\n",
        "- a full merged table (original CSV + `dbscan_cluster`).\n",
        "\n",
        "### Inputs\n",
        "**Required**\n",
        "- `EMBED_JSON_PATH`: JSON records containing at least:\n",
        "  - `commentId`\n",
        "  - `embedding` (vector stored as a Python list)\n",
        "- `CSV_PATH`: original comments CSV (used to build the FULL output)\n",
        "\n",
        "### Outputs\n",
        "- `DBSCAN_CSV_PATH`: lightweight CSV with:\n",
        "  - `commentId`, `dbscan_cluster`\n",
        "- `DBSCAN_FULL_CSV_PATH`: full CSV with:\n",
        "  - all original columns from `CSV_PATH` + `dbscan_cluster`\n",
        "\n",
        "### DBSCAN settings (cosine distance)\n",
        "- `eps`: neighborhood radius in cosine-distance space  \n",
        "- `min_samples`: minimum number of points required to form a core cluster\n",
        "- Label `-1` indicates **noise** (unclustered points)"
      ],
      "metadata": {
        "id": "wBjeI3eXsAMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 4.1 Load embeddings JSON ----\n",
        "df_embed = pd.read_json(EMBED_JSON_PATH, orient=\"records\")\n",
        "df_embed[\"commentId\"] = df_embed[\"commentId\"].astype(str)\n",
        "\n",
        "print(\"df_embed rows:\", len(df_embed))\n",
        "print(\"df_embed columns:\", df_embed.columns.tolist())\n",
        "\n",
        "# ---- 4.2 Build DBSCAN input matrix X ----\n",
        "X = np.stack(df_embed[\"embedding\"].to_numpy()).astype(np.float32)\n",
        "print(\"X shape:\", X.shape)\n",
        "\n",
        "# ---- 4.3 Load original CSV (required for FULL merged export) ----\n",
        "df_full = pd.read_csv(CSV_PATH)\n",
        "df_full[\"commentId\"] = df_full[\"commentId\"].astype(str)\n",
        "\n",
        "print(\"df_full rows:\", len(df_full))\n",
        "print(\"df_full columns (first 12):\", df_full.columns.tolist()[:12])\n",
        "\n",
        "# ---- 4.4 Run DBSCAN ----\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Tunable parameters\n",
        "EPS = 0.05  # cosine-distance radius\n",
        "MIN_SAMPLES = 5  # minimum points to form a cluster\n",
        "\n",
        "db = DBSCAN(\n",
        "    eps=EPS,\n",
        "    min_samples=MIN_SAMPLES,\n",
        "    metric=\"cosine\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"Fitting DBSCAN ...\")\n",
        "db_labels = db.fit_predict(X)\n",
        "print(\"DBSCAN finished.\")\n",
        "\n",
        "# Summary: cluster sizes (including noise = -1)\n",
        "unique_labels, counts = np.unique(db_labels, return_counts=True)\n",
        "\n",
        "print(\"Top clusters by size (including noise = -1):\")\n",
        "for lbl, cnt in sorted(zip(unique_labels, counts), key=lambda x: -x[1])[:15]:\n",
        "    print(f\"  cluster {lbl}: {cnt} comments\")\n",
        "\n",
        "# Attach labels\n",
        "df_embed[\"dbscan_cluster\"] = db_labels\n",
        "df_embed[[\"commentId\", \"dbscan_cluster\"]].head()"
      ],
      "metadata": {
        "id": "o-l3A8hysU3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 4.5 Merge dbscan_cluster back onto the original CSV ----\n",
        "df_dbscan = df_embed[[\"commentId\", \"dbscan_cluster\"]].copy()\n",
        "df_dbscan[\"commentId\"] = df_dbscan[\"commentId\"].astype(str)\n",
        "\n",
        "df_full_out = df_full.merge(df_dbscan, on=\"commentId\", how=\"left\")\n",
        "\n",
        "print(\"Rows after merge:\", len(df_full_out))\n",
        "print(\"Cluster columns in output:\")\n",
        "print([c for c in df_full_out.columns if \"cluster\" in c])\n",
        "\n",
        "df_dbscan.to_csv(DBSCAN_CSV_PATH, index=False)\n",
        "print(\"Saved lightweight DBSCAN CSV to:\", DBSCAN_CSV_PATH)\n",
        "\n",
        "df_full_out.to_csv(DBSCAN_FULL_CSV_PATH, index=False)\n",
        "print(\"Saved FULL CSV to:\", DBSCAN_FULL_CSV_PATH)\n",
        "\n",
        "df_full_out[[\"commentId\", \"dbscan_cluster\"]].head()"
      ],
      "metadata": {
        "id": "-q0rrvn1vzR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. DBSCAN post-processing (exports)\n",
        "\n",
        "From `df_full_out` (original CSV + `dbscan_cluster`), export:\n",
        "\n",
        "1. Noise only (`dbscan_cluster == -1`)  \n",
        "2. One representative per non-noise cluster (most complete row; ties by longer text)  \n",
        "3. All rows in clusters with size > 5  \n",
        "4. Noise + representatives\n"
      ],
      "metadata": {
        "id": "zowH7SNlvzC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 5.1 Split noise vs non-noise ----\n",
        "df_noise = df_full_out[df_full_out[\"dbscan_cluster\"] == -1].copy()\n",
        "df_nonnoise = df_full_out[df_full_out[\"dbscan_cluster\"] != -1].copy()\n",
        "\n",
        "print(\"Noise rows:\", len(df_noise))\n",
        "print(\"Non-noise rows:\", len(df_nonnoise))"
      ],
      "metadata": {
        "id": "28xPG6jPwsYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 5.2 Representatives per cluster ----\n",
        "info_cols = [\n",
        "    \"title\", \"trackingNbr\", \"organizationName\", \"firstName\", \"lastName\",\n",
        "    \"city\", \"stateProvinceRegion\", \"country\", \"combinedText\",\n",
        "]\n",
        "info_cols = [c for c in info_cols if c in df_nonnoise.columns]\n",
        "\n",
        "# Treat empty strings as missing for scoring\n",
        "for c in info_cols:\n",
        "    df_nonnoise[c] = df_nonnoise[c].replace(r\"^\\s*$\", np.nan, regex=True)\n",
        "\n",
        "df_nonnoise[\"info_nonnull\"] = df_nonnoise[info_cols].notna().sum(axis=1) if info_cols else 0\n",
        "df_nonnoise[\"text_len\"] = (\n",
        "    df_nonnoise[\"combinedText\"].fillna(\"\").astype(str).str.len()\n",
        "    if \"combinedText\" in df_nonnoise.columns else 0\n",
        ")\n",
        "\n",
        "df_reps = (\n",
        "    df_nonnoise\n",
        "    .sort_values([\"dbscan_cluster\", \"info_nonnull\", \"text_len\"], ascending=[True, False, False])\n",
        "    .groupby(\"dbscan_cluster\", as_index=False)\n",
        "    .first()\n",
        "    .drop(columns=[\"info_nonnull\", \"text_len\"], errors=\"ignore\")\n",
        ")\n",
        "\n",
        "print(\"Representatives:\", len(df_reps))"
      ],
      "metadata": {
        "id": "KB3NFhXQwwvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 5.3 Clusters with size > 5 (full content) ----\n",
        "cluster_counts = df_nonnoise[\"dbscan_cluster\"].value_counts()\n",
        "big_clusters = cluster_counts[cluster_counts > 5].index\n",
        "\n",
        "df_big = df_nonnoise[df_nonnoise[\"dbscan_cluster\"].isin(big_clusters)].copy()\n",
        "\n",
        "print(\"Big clusters (>5):\", len(big_clusters))\n",
        "print(\"Rows in big clusters:\", len(df_big))\n"
      ],
      "metadata": {
        "id": "wjo1PnLbwy9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 5.4 Noise + representatives ----\n",
        "df_noise_plus_reps = pd.concat([df_noise, df_reps], ignore_index=True)\n",
        "print(\"Rows in noise + reps:\", len(df_noise_plus_reps))\n"
      ],
      "metadata": {
        "id": "qKN5NcsVw1Ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 5.5 Save outputs ----\n",
        "tag = f\"eps{EPS:g}_min{MIN_SAMPLES}\"\n",
        "\n",
        "noise_path      = os.path.join(DATA_DIR, f\"{PREFIX}_dbscan_{tag}_noise_only.csv\")\n",
        "reps_path       = os.path.join(DATA_DIR, f\"{PREFIX}_dbscan_{tag}_cluster_representatives.csv\")\n",
        "big_path        = os.path.join(DATA_DIR, f\"{PREFIX}_dbscan_{tag}_clusters_gt5_full.csv\")\n",
        "noise_reps_path = os.path.join(DATA_DIR, f\"{PREFIX}_dbscan_{tag}_noise_plus_reps.csv\")\n",
        "\n",
        "df_noise.to_csv(noise_path, index=False)\n",
        "df_reps.to_csv(reps_path, index=False)\n",
        "df_big.to_csv(big_path, index=False)\n",
        "df_noise_plus_reps.to_csv(noise_reps_path, index=False)\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(\"1) Noise only:\", noise_path)\n",
        "print(\"2) Representatives:\", reps_path)\n",
        "print(\"3) Clusters > 5 (full):\", big_path)\n",
        "print(\"4) Noise + reps:\", noise_reps_path)"
      ],
      "metadata": {
        "id": "x0Kjh1nYw3Fz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}